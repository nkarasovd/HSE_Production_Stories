# Reinforcement Learning для траектории полного обхода динамической среды

## Задача

Исследовать среду с ограниченным углом обзора.

## Параметры среды

- width: 20 
  
- height: 20
  
- max_rooms: 3 
  
- min_room_xy: 5 

- max_room_xy: 10 

- observation_size: 11

- vision_radius: 5

## PPO, архитектура модели
- Conv2D(in_channels=4, out_channels=16, kernel_size=3, stride=2)
- Conv2D(in_channels=32, out_channels=32, kernel_size=3, stride=2)
- Conv2D(in_channels=32, out_channels=32, kernel_size=3, stride=1)
- Linear(32)
- Linear(out_dim)

Размер выхода модели `out_dim` равен либо `action_dim` (Actor), либо `1` (Critic).

## Функции награды
В ходе экспериментов были опробованы 3 функции награды.

- Базовая награда `basic`: доля новых исследованных клеток.
  Проблема такой награды – с уменьшением числа неисследованных клеток
  такая награда становится очень маленькой. Агенту сложнее найти
  оставшиеся клетки. Поэтому были выбраны две модификации.

- Награда `reward_new_explored`, мотивируем агента как можно быстрее изучить все помещение:
  - `-1.0` за столкновение;
  - `-0.5` если не обнаружил новых клеток;
  - `0.1 + 20 * (total_explored / total_cells)` если обнаружены новые клетки.

- Награда `reward_new_cell`. Награждаем агента за исследование новых клеток.
  При этом награду за исследование даем только тогда,
  когда он проходит по новым клеткам. 
  Мотивация: хотим, чтобы агент научился исследовать 
  помещение, при этом повторно не проходить по клеткам.
    
  - `-1.0` за столкновение;
  - `-0.5` если не обнаружил новых клеток;
  - `0.1 + 20 * (total_explored / total_cells)` если прошел по новой клетке.

## Результаты

Все результаты экспериментов логируются с помощью [WandB](https://wandb.ai/nkarasovd/prod_stories_task_5?workspace=user-nkarasovd).
Все параметры экспериментов есть в соответсвующих `run-ах`. 
Если посмотреть на получившиеся траектории, то лучше всего исследовать 
карту научился агент с наградой `reward_new_explored`. 

Возможное улучшение награды `reward_new_cell` в том, чтобы
дополнительно наложить условие: давать награду не только тогда, 
когда он зашел на новую клетку, но чтобы при этом агент обнаружил
новые клетки вокруг себя. 

Пробовал различные гиперпараметры экспериментов, золотой середины не нашел.
Особенность наград `reward_new_explored` и `reward_new_cell` – 
большое значение `value function loss`. Неприятный момент в том, 
что итоговый лосс фактически определяется `value function loss`.
При этом агент с наградой `reward_new_explored` показывает результаты 
лучше, чем агент с базовой наградой, при которой все функции потерь
ведут себя "адекватно". При всех видах награды энтропия убывает. 

